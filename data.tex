\section{\label{sec:data}Procedure for Data Analysis}


The broad steps for analysis of a specific reaction in the \desg{g12} data are:
\begin{enumerate}
    \item determine event selection cuts to be used on a small subset of the data,
    \item skim the ``cooked'' data and produce an ntuple which will include the four-momenta and a few other parameters needed for the final analysis,
    \item tune the analysis on the data to get the final yields,
    \item run simulations through the tracker/digitizer (\prog{gsim}), smearing (\prog{gpp}), track reconstruction (\prog{a1c}), and the final analysis programs to obtain efficiencies and acceptances,
    \item calculate the beam flux corresponding to the data analyzed,
    \item tie the yields, acceptances and flux together to come up with a final answer and an absolute scaling.
\end{enumerate}
One or more of these steps, or parts of these steps, may not be necessary for a given analysis and they do not have to be done this order. This section discusses where to find the various programs and files needed for each of these steps.

\subsection{\label{sec:data.cook}``Pass1'' Cooking and Data Reconstruction}

This section describes the process of reconstructing the raw data into almost-physics-ready form. Broadly speaking, the output of this procedure is the three-momenta of the detected final-state particles and various supporting measurements such as timing or energy deposit which is used for particle identification.

The reconstruction program \texttt{a1c} requires an environment variable to point to the correct run index for this run period. This can be set in a bash shell using the command:
\begin{verbatim}
export CLAS_CALDB_RUNINDEX=calib_user.RunIndexg12
\end{verbatim}
The command to cook the gsim bos file using the \texttt{a1c} program is:
\begin{verbatim}
a1c -T4 -sa -ct1930 -cm0 -cp0 -X0 -d1 -F -P0x1bff -z0,0,-90 \
  -Aprlink_tg-90pm30.bos -o3pi.a1c 3pi.gpp
\end{verbatim}
which will produce BOS files similar to the ``cooked'' data. This is the same command to be used to \emph{recook} raw data files. The options shown here will be slightly different for simulated data since \desg{CLAS} does not treat simulation in exactly the same way as real data. This is a legacy issue that has been well documented in previous experiments and we will merely say that one will have to add the option ``\verb+-X0+'' when running the \prog{a1c} reconstruction program. This is mentioned again in Sec.~\ref{sec:sim.recon} where we discuss the reconstruction of simulated data.

There was only one ``physics'' pass of the \desg{g12} data, labeled \texttt{pass1}, which represents 99\% of the reconstructable data taken. It was determined that tracking down that last 1\% was not worth the effort. On JLab's common user environment (\texttt{CUE}), the cooked data can be found here:
\begin{center}
    \texttt{/mss/clas/g12/production/pass1/bos}
\end{center}
and the raw data can be found here:
\begin{center}
    \texttt{/mss/clas/g12/data/}
\end{center}
Please see JLab's SciComp help pages on how to get these files off the mass storage system. The source code for pass1 was consistent with revision 4715 in the Subversion repository found here:
\begin{center}
    \url{https://jlabsvn.jlab.org/svnroot/clas/trunk}
\end{center}
which was linked against the 2005 version of the \prog{CERNLIB}.

\FloatBarrier





\subsection{\label{sec:data.pass1}Obtaining Reconstructed Data}

All reconstructed data resides in BOS files on the tape-silo at JLab under the directory:
\begin{align}
\texttt{/mss/clas/g12/production/pass1/bos} \nonumber
\end{align}
which contains the following subdirectories or ``categories'' used for event-sorting:
\begin{description}
    \item[1-1ckaon1ctrk] Events which have at least 2 charged tracks, one of which is a ``possible kaon.'' A possible kaon is either a track that the \abbr{PART} bank says is a kaon, or a high-momentum charged pion ($> 2.0$~GeV), or a really high momentum proton ($> 3.0$~GeV). The idea of this selection is to leave no kaon behind.
    \item[2-2pos1neg\_not\_1ckaon1ctrk] Two-positive and one-negative ($++-$) inclusive events which are \emph{not} included in \emph{1-1ckaon1ctrk}. So, for example, if you wanted all ($++-$) events you would have to use both this category and \emph{1-1ckaon1ctrk}.
    \item[3-2ctrk\_not\_2pos1neg\_1ckaon1ctrk] Events with 2 or more charged tracks which do not qualify for either \emph{1-1ckaon1ctrk} or \emph{2-2pos1neg\_not\_1ckaon1ctrk}.
    \item[4-not\_2ctrk\_2pos1neg\_1ckaon1ctrk] Physics events that do not fit into categories 1, 2, or 3.
    \item[5-other] Non-physics events which may include scalers and such.
    \item[6-1lepton] Redundant set of all events with a single ``possible lepton'' according to the
    \begin{align}
        \texttt{ClasParticle::isMaybeLepton()} \nonumber
    \end{align}
    method in the ClasEvent analysis suite.
    \item[7-4ctrk] Redundant set of all four-charged track events.
    \item[8-ppbar] Redundant set of all proton, anti-proton events according to the PART bank.
\end{description}
Note that the first five of these categories are \emph{mutually exclusive and complete} while the last three are completely redundant and are provided for convenience. Non-physics events are included in the fifth (``other'') skim. If one wishes to synchronize these events with their data, they have some options. Perhaps the easiest is to process the raw data which is completely ordered. Timestamps could be used instead, provided they exist in the non-physics skim.

\paragraph{This bears repeating since it is a non-standard sorting of the cooked data:} The five categories numbered 1 through 5 listed above consist of one and only one of \emph{every} event recorded by the data acquisition during the g12 run period for the set of run which were deemed ``good.'' A specific single event will only be found in one of the first five categories. If you want to run on events that are described by category number 4 for example, you will have to include the data in categories 1 through 3 since these will also satisfy category 4 by design and by definition.


The total size of the reconstructed data is 211~TiB consisting of 26.2 billion triggers. The secondary skims (6--8) total 31~TiB. The breakdown of the size of the skims is shown in Table~\ref{tab:skimsize}.

\input{tables/skimsize} % tab:skimsize


Typical analyses with \desg{g12} will start with the particles as identified in the \abbr{PART} bank as default in the \prog{ClasEvent} analysis suite. This provides the basic four-momenta of the tracks and their identification which is based on mass calculated from the time-of-flight. The photon associated with the event is then taken from the \abbr{TAGR} bank and the one closest in-time with the tracks is usually taken.

For a deeper analysis, one can get information for specific hits in a given subsystem by following the pointers in the \abbr{PART} and \abbr{TBID} bank to the \abbr{TBTR}, \abbr{SCRC}, \abbr{ECHB} or other similar banks. Most of the relevant banks needed for this type of investigation are included in the cooked data though several raw banks were dropped to save space. To recover the missing banks one would have to go back to the raw data and recook using the \prog{a1c} program though we anticipate this will be a rare event.


The following banks were written out to tape in the pass1 reconstructed data: \abbr{CC CC01 CCPB CCRC CL01 DCPB DSTC EC EC1 EC1R ECHB ECPB ECPC ECPI ECS EPIC EVNT FBPM HBER HBID HBTR HEAD HEVT HLS LCPB MVRT PART RGLK S1ST SC SC1 SCPB SCR SCRC SCS ST1 STN0 STN1 STPB STR STSN SYNC TAGE TAGI TAGR TBER TBID TBTR TDPL TGBI TGPB TGS TRGS TRKS TRL1}. Note that the SEB-related banks (\abbr{EVNT} etc.) were written out. These may be used for any analysis though care should be taken not to mix these with the \abbr{PART}-related series of banks except where there is an apparent redundancy.


\input{data/corrections}

\input{data/lepton}
